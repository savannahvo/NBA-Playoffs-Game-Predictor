{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b2e841c4-2c41-480e-baf8-3f8e38a9a341",
   "metadata": {},
   "source": [
    "## Data Processing\n",
    "\n",
    "This notebook processes modeled NBA playoff datasets from 2015–2025, preparing them for machine learning model training and evaluation.  \n",
    "It applies data cleaning, feature preprocessing, and structured train, validation, and test splits to ensure consistency and prevent data leakage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "680e73a7-1e40-4853-a0e0-c7ace89ddcf0",
   "metadata": {},
   "source": [
    "#### Combining All Modeled Datasets\n",
    "This section merges all playoff round/game modeling outputs into a single master dataset. Combining these datasets into one file ensures that all features and game contexts are available for consistent preprocessing and model training.\n",
    "\n",
    "##### Overview of Steps:\n",
    "1. **Import Libraries**: Load `pandas` for data handling and `glob` for file path management.\n",
    "2. **Define File Paths**: List the CSV outputs for each playoff round and game scenario.\n",
    "3. **Load and Combine**: Read each file into a DataFrame and concatenate them into one dataset.\n",
    "4. **Save Combined Dataset**: Export the merged dataset as `all_modeled_playoff_games.csv` for use in preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8c0a791c-015c-421b-84c1-a46b36292c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "# Load file paths for each modeling dataset\n",
    "file_paths = [\n",
    "    \"../data/processed/modeling_round1_game1.csv\",\n",
    "    \"../data/processed/modeling_round1_game2.csv\",\n",
    "    \"../data/processed/modeling_round1_games3to7.csv\",\n",
    "    \"../data/processed/modeling_round2_game1.csv\",\n",
    "    \"../data/processed/modeling_round2_game2to7.csv\",\n",
    "    \"../data/processed/modeling_round3_game1.csv\",\n",
    "    \"../data/processed/modeling_round3_games2to7.csv\",\n",
    "    \"../data/processed/modeling_round4_game1.csv\",\n",
    "    \"../data/processed/modeling_round4_games2to7.csv\"\n",
    "]\n",
    "\n",
    "# Load and combine \n",
    "dfs = []\n",
    "for file in file_paths:\n",
    "    df = pd.read_csv(file)\n",
    "    dfs.append(df)\n",
    "\n",
    "all_games = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "# Export the final dataset\n",
    "all_games.to_csv(\"../data/processed/all_modeled_playoff_games.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "848535c7-ccb8-434a-b45e-365aa63006da",
   "metadata": {},
   "source": [
    "#### Build Processed Feature Sets\n",
    "We split the combined dataset by season, remove non-predictive identifiers, and apply a preprocessing pipeline:\n",
    "numeric features → mean imputation + standard scaling; categorical features → one-hot encoding with unknown handling.  \n",
    "Outputs are saved for downstream modeling.\n",
    "\n",
    "**Overview of Steps:**\n",
    "1. **Import Libraries**: Use `pandas` for data handling and `sklearn` for preprocessing.\n",
    "2. **Load Combined Dataset**: Read `all_modeled_playoff_games.csv` file.\n",
    "3. **Season Split**:  \n",
    "   - Train: 2015–2022  \n",
    "   - Validation: 2023–2024  \n",
    "   - Test: 2025\n",
    "4. **Drop Non-Predictive Columns**: Remove `homeTeam`, `awayTeam`, `gameDate` (identifiers).\n",
    "5. **Separate Features/Target**: Target is `homeWin`.\n",
    "6. **Detect Column Types**: Identify numeric and categorical columns.\n",
    "7. **Fit Preprocessor**: `ColumnTransformer` = numeric(impute+scale) + categorical(one-hot).\n",
    "8. **Transform Splits**: Apply the fitted preprocessor to train/val/test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4d2b7c85-04fa-4cf4-90df-aa8117b5fbfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "import os, json\n",
    "import pandas as pd\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "import joblib\n",
    "\n",
    "# Load all modeled playoff games file\n",
    "all_models  = \"../data/processed/all_modeled_playoff_games.csv\"\n",
    "processed_dir = \"../data/processed\"\n",
    "model_dir = \"../models\"\n",
    "os.makedirs(processed_dir, exist_ok=True)\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Load Combined Dataset\n",
    "df = pd.read_csv(all_models)\n",
    "\n",
    "# Season Split\n",
    "train_df = df[df[\"season\"].between(2015, 2022)].copy()\n",
    "val_df   = df[df[\"season\"].between(2023, 2024)].copy()\n",
    "test_df  = df[df[\"season\"] == 2025].copy()\n",
    "\n",
    "# Drop Non-Predictive Columns\n",
    "cols_to_drop = [\"homeTeam\", \"awayTeam\", \"gameDate\"]\n",
    "for _d in (train_df, val_df, test_df):\n",
    "    _d.drop(columns=[c for c in cols_to_drop if c in _d.columns], inplace=True, errors=\"ignore\")\n",
    "\n",
    "# Separate Target and Features\n",
    "target = \"homeWin\"\n",
    "y_train = train_df[target]; X_train = train_df.drop(columns=[target])\n",
    "y_val   = val_df[target];   X_val   = val_df.drop(columns=[target])\n",
    "y_test  = test_df[target];  X_test  = test_df.drop(columns=[target])\n",
    "\n",
    "# Detect Column Types\n",
    "numeric_cols = X_train.select_dtypes(include=[\"number\"]).columns.tolist()\n",
    "categorical_cols = X_train.select_dtypes(include=[\"object\", \"category\", \"bool\"]).columns.tolist()\n",
    "\n",
    "# Build Preprocessor\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", Pipeline([\n",
    "            (\"imputer\", SimpleImputer(strategy=\"mean\")),\n",
    "            (\"scaler\", StandardScaler())\n",
    "        ]), numeric_cols),\n",
    "        (\"cat\", OneHotEncoder(drop=\"first\", handle_unknown=\"ignore\"), categorical_cols)\n",
    "    ],\n",
    "    remainder=\"drop\"\n",
    ")\n",
    "\n",
    "# Fit on TRAIN only, then transform all splits\n",
    "preprocessor.fit(X_train)\n",
    "\n",
    "X_train_proc = preprocessor.transform(X_train)\n",
    "X_val_proc   = preprocessor.transform(X_val)\n",
    "X_test_proc  = preprocessor.transform(X_test)\n",
    "\n",
    "# Retrieve post-transform feature names\n",
    "def get_feature_names_out(preprocessor, num_cols, cat_cols):\n",
    "    num_feats = [f\"num__{c}\" for c in num_cols]\n",
    "    cat_encoder = preprocessor.named_transformers_[\"cat\"]\n",
    "    cat_feats = cat_encoder.get_feature_names_out(cat_cols).tolist()\n",
    "    return num_feats + cat_feats\n",
    "\n",
    "feature_names = get_feature_names_out(preprocessor, numeric_cols, categorical_cols)\n",
    "\n",
    "# Convert to DataFrames with consistent column order\n",
    "import numpy as np\n",
    "X_train_df = pd.DataFrame(X_train_proc, columns=feature_names, index=X_train.index)\n",
    "X_val_df   = pd.DataFrame(X_val_proc,   columns=feature_names, index=X_val.index)\n",
    "X_test_df  = pd.DataFrame(X_test_proc,  columns=feature_names, index=X_test.index)\n",
    "\n",
    "# Save processed matrices & labels\n",
    "X_train_df.to_csv(f\"{processed_dir}/X_train_processed.csv\", index=False)\n",
    "X_val_df.to_csv(f\"{processed_dir}/X_val_processed.csv\", index=False)\n",
    "X_test_df.to_csv(f\"{processed_dir}/X_test_processed.csv\", index=False)\n",
    "\n",
    "y_train.to_csv(f\"{processed_dir}/y_train.csv\", index=False)\n",
    "y_val.to_csv(f\"{processed_dir}/y_val.csv\", index=False)\n",
    "y_test.to_csv(f\"{processed_dir}/y_test.csv\", index=False)\n",
    "\n",
    "# Save preprocessor and feature names \n",
    "joblib.dump(preprocessor, f\"{model_dir}/final_preprocessor.pkl\")\n",
    "with open(f\"{model_dir}/feature_names.json\", \"w\") as f:\n",
    "    json.dump(feature_names, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6c8cfdf-1bcc-471a-9c69-7ec8d61bee25",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
